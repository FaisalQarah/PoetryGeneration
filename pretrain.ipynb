{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "084954b8-6733-4229-95b4-13e8e6131ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/ffq/.cache/huggingface/datasets/text/default-4bb4e3c62e0bcdd7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4935439bd249328d17ffbd92bbdd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943213c494a34dde8ab7322c1f7698f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/ffq/.cache/huggingface/datasets/text/default-4bb4e3c62e0bcdd7/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e415252c2a40268890aabed8ee0ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2090907\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 210/210 [00:03<00:00, 69.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('PoemBERT/tokenizer_config.json',\n",
       " 'PoemBERT/special_tokens_map.json',\n",
       " 'PoemBERT/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the tokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import glob\n",
    "\n",
    "#loading any bert SentencePiece tokenizer to work as a base for the new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('faisalq/bert-base-arabic-senpiece')\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset('text', data_files=['pretrain_text.txt'])\n",
    "\n",
    "display(dataset)\n",
    "\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(dataset['train']), batch_size)):\n",
    "        yield dataset['train'][i: i +batch_size]['text']\n",
    "bert_tokenizer = tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), \n",
    "                                                   vocab_size=70000\n",
    "                                                   #, special_tokens=['[CLS]', '[PAD]','[SEP]','[UNK]','[MASK]']\n",
    "                                                  )\n",
    "bert_tokenizer.save_pretrained('PoetBERT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e8cdcd-050c-4b35-85be-aa8b9ad5cb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423a0e968438406bb48615ab5ea52428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/ffq/.cache/huggingface/datasets/text/default-79afbc99aca1de7f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac052e8492ab4f3dac623bd6115788a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/132392608 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 132392608\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizing the whole text corpus\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "from datasets import load_dataset\n",
    "import glob\n",
    "import tokenizers\n",
    "from transformers import Trainer, TrainingArguments, LineByLineTextDataset, BertModel\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('faisalq/bert-base-arapoembert')\n",
    "tokenizer = AutoTokenizer.from_pretrained('PoetBERT/')\n",
    "max_seq_length = 32\n",
    "\n",
    "dataset = load_dataset('text', data_files=['pretrain_text.txt'])\n",
    "\n",
    "\n",
    "\n",
    "def encode_with_truncation(examples):\n",
    "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_seq_length, return_special_tokens_mask=True)\n",
    "\n",
    "\n",
    "dataset = dataset[\"train\"].map(encode_with_truncation, batched=True) #, load_from_cache_file=True\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "dataset = dataset.remove_columns([\"text\"])\n",
    "dataset.save_to_disk(\"tokenized_dataset/\")\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be026e91-c580-4b44-878b-3a52027c3aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cae0d0-7ed8-44e0-a275-2babb142a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretraining the model\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "from datasets import load_dataset\n",
    "import glob\n",
    "import tokenizers\n",
    "from transformers import Trainer, TrainingArguments, LineByLineTextDataset, BertModel\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_from_disk(\"tokenized_dataset/\")\n",
    "\n",
    "max_seq_length = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained('PoetBERT/')\n",
    "config = BertConfig( vocab_size = 70000, \n",
    "                    hidden_size = 768, \n",
    "                    num_hidden_layers = 12,\n",
    "                    num_attention_heads = 12,\n",
    "                    max_position_embeddings = 512)\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "display(model.num_parameters())\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True,\n",
    "                                               mlm_probability=0.15)\n",
    "epochs = 1000\n",
    "save_steps = 50_000 #save checkpoint every 10000 steps\n",
    "batch_size = 512 #256 #64 # i don't think you can run larger batch_size on an 80GB-GPU\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'PoetBERT/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs = epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    save_steps = save_steps,\n",
    "    save_total_limit = 5, #only save the last 5 checkpoints\n",
    "    fp16=True,\n",
    "    # tf32 = True,\n",
    "    learning_rate = 5e-5,  # 5e-5 is the default\n",
    "    logging_steps = 10_000,\n",
    "    # gradient_accumulation_steps=2,\n",
    "    # gradient_checkpointing=True,\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()\n",
    "trainer.save_model('PoetBERT/final/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0941730d-8d28-415f-8ee4-2b7c7577c9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a6382-8f7b-4392-b1d8-95cb5865232a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8df27-ba7f-4663-868a-5cbf8384b6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
